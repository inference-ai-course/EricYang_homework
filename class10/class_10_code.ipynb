{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG Agent with LangGraph: Complete Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detail Langgraph Agent learning, you can check this repo: https://github.com/ScottLL/langgraph_lib\n",
    "\n",
    "## 1. Introduction to RAG Agents\n",
    "\n",
    "### What is a RAG Agent?\n",
    "\n",
    "A **RAG (Retrieval-Augmented Generation) Agent** combines the power of Large Language Models (LLMs) with external knowledge retrieval capabilities. Unlike standalone LLMs that are limited to their training data, RAG agents can:\n",
    "\n",
    "- **Access up-to-date information** from external documents\n",
    "- **Perform multi-step reasoning** with retrieved context\n",
    "- **Maintain conversation state** across interactions\n",
    "- **Use specialized tools** for document retrieval\n",
    "\n",
    "### Key Components We'll Build:\n",
    "\n",
    "1. **Document Processor**: Loads and chunks PDF documents\n",
    "2. **Vector Store**: Creates searchable embeddings of document chunks\n",
    "3. **Retrieval Tool**: Searches for relevant information\n",
    "4. **LLM Agent**: Reasons with retrieved information\n",
    "5. **Graph Orchestrator**: Manages the agent workflow\n",
    "\n",
    "### Architecture Overview:\n",
    "\n",
    "```\n",
    "User Query → LLM Agent → Retrieval Tool → Vector Store → PDF Documents\n",
    "     ↑                        ↓\n",
    "     └── Final Response ← LLM Agent ← Retrieved Context\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "First, let's install and import all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (0.3.22)\n",
      "Requirement already satisfied: langchain-community in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (0.3.25)\n",
      "Requirement already satisfied: langgraph in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (0.5.1)\n",
      "Requirement already satisfied: chromadb in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (1.0.15)\n",
      "Requirement already satisfied: pypdf in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (5.6.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain_chroma in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (0.2.4)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-openai) (1.86.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-community) (3.12.12)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langchain-community) (2.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langgraph) (2.1.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langgraph) (0.5.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langgraph) (0.1.72)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
      "Requirement already satisfied: filelock in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "! pip install langchain langchain-openai langchain-community langgraph chromadb pypdf python-dotenv langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from operator import add as add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Load environment variables (make sure you have OPENAI_API_KEY in your .env file)\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**💡 What's happening here?**\n",
    "- **LangChain**: Framework for building LLM applications\n",
    "- **LangGraph**: Creates state-based agent workflows\n",
    "- **ChromaDB**: Vector database for storing embeddings\n",
    "- **OpenAI**: LLM and embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Initialization \n",
    "\n",
    "Let's set up our LLM and embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 LLM initialized: GPT-4o\n",
      "🔍 Embeddings initialized: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Language Model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\", \n",
    "    temperature=0  # Low temperature for more deterministic responses\n",
    ")\n",
    "\n",
    "# Initialize the Embedding Model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Efficient embedding model\n",
    ")\n",
    "\n",
    "print(\"🤖 LLM initialized: GPT-4o\")\n",
    "print(\"🔍 Embeddings initialized: text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🔍 Key Parameters:**\n",
    "- **Temperature = 0**: Makes responses more deterministic and factual\n",
    "- **text-embedding-3-small**: Cost-effective embedding model for document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Loading and Processing\n",
    "\n",
    "Now let's load and process our PDF document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Found PDF: Stock_Market_Performance_2024.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your PDF document\n",
    "pdf_path = \"Stock_Market_Performance_2024.pdf\"\n",
    "\n",
    "# Safety check: Verify PDF exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    raise FileNotFoundError(f\"❌ PDF file not found: {pdf_path}\")\n",
    "    \n",
    "print(f\"📄 Found PDF: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF loaded successfully!\n",
      "📊 Document contains 9 pages\n",
      "📝 First page preview: Stock Market Performance in 2024\n",
      "U.S. Market Overview\n",
      "The year 2024 was a remarkably strong one for equities, with the U.S. stock market extending the\n",
      "robust gains seen in the prior year. The benchmar...\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "pdf_loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "try:\n",
    "    pages = pdf_loader.load()\n",
    "    print(f\"✅ PDF loaded successfully!\")\n",
    "    print(f\"📊 Document contains {len(pages)} pages\")\n",
    "    \n",
    "    # Preview first page content (first 200 characters)\n",
    "    if pages:\n",
    "        print(f\"📝 First page preview: {pages[0].page_content[:200]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading PDF: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🔄 What's happening?**\n",
    "- **PyPDFLoader**: Extracts text from each page of the PDF\n",
    "- **Error handling**: Ensures robust document loading\n",
    "- **Content preview**: Shows what was extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Chunking \n",
    "\n",
    "Large documents need to be split into smaller, manageable chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Original pages: 9\n",
      "🔍 Total chunks created: 24\n",
      "📏 Average chunk size: 868 characters\n",
      "\n",
      "📝 Sample chunk:\n",
      "Stock Market Performance in 2024\n",
      "U.S. Market Overview\n",
      "The year 2024 was a remarkably strong one for equities, with the U.S. stock market extending the\n",
      "robust gains seen in the prior year. The benchmark S&P 500 index delivered roughly a 25% total\n",
      "return for 2024 (around +23% in price terms)\n",
      ". This ma...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Maximum characters per chunk\n",
    "    chunk_overlap=200     # Overlap between chunks to maintain context\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "pages_split = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"📄 Original pages: {len(pages)}\")\n",
    "print(f\"🔍 Total chunks created: {len(pages_split)}\")\n",
    "print(f\"📏 Average chunk size: {sum(len(chunk.page_content) for chunk in pages_split) // len(pages_split)} characters\")\n",
    "\n",
    "# Preview a sample chunk\n",
    "if pages_split:\n",
    "    print(f\"\\n📝 Sample chunk:\\n{pages_split[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🎯 Chunking Strategy:**\n",
    "- **Chunk size (1000)**: Balance between context and retrieval precision\n",
    "- **Overlap (200)**: Prevents information loss at chunk boundaries\n",
    "- **Recursive splitting**: Maintains semantic coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vector Store Creation\n",
    "\n",
    "Create a ChromaDB vector store to enable semantic search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChromaDB vector store created successfully!\n",
      "📊 Indexed 24 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Set up ChromaDB configuration\n",
    "persist_directory = \"./chroma_db\"  # Local directory for vector store\n",
    "collection_name = \"stock_market\"   # Collection name for our documents\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "    print(f\"📁 Created directory: {persist_directory}\")\n",
    "\n",
    "try:\n",
    "    # Create the ChromaDB vector store\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=pages_split,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    print(f\"✅ ChromaDB vector store created successfully!\")\n",
    "    print(f\"📊 Indexed {len(pages_split)} document chunks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up ChromaDB: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**💾 Vector Store Benefits:**\n",
    "- **Semantic search**: Find similar content, not just keyword matches\n",
    "- **Persistent storage**: Database saves to disk for reuse\n",
    "- **Efficient retrieval**: Fast similarity search across large documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building the Retrieval Tool\n",
    "\n",
    "Create a retriever and wrap it in a tool that the agent can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Retriever configured to return top 5 similar chunks\n"
     ]
    }
   ],
   "source": [
    "# Create the retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  # Return top 5 most similar chunks\n",
    ")\n",
    "\n",
    "print(f\"🔍 Retriever configured to return top 5 similar chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Retrieval tool created successfully!\n",
      "📋 Tool description: This tool searches and returns information from the Stock Market Performance 2024 document.\n",
      "Use this tool when you need to find specific information about stock market data, trends, or analysis.\n"
     ]
    }
   ],
   "source": [
    "# Define the retrieval tool using LangChain's @tool decorator\n",
    "@tool\n",
    "def retriever_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool searches and returns information from the Stock Market Performance 2024 document.\n",
    "    Use this tool when you need to find specific information about stock market data, trends, or analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform the similarity search\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    # Handle case where no documents are found\n",
    "    if not docs:\n",
    "        return \"I found no relevant information in the Stock Market Performance 2024 document.\"\n",
    "    \n",
    "    # Format the retrieved documents\n",
    "    results = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        results.append(f\"Document {i+1}:\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "# Test the retrieval tool\n",
    "print(\"🛠️ Retrieval tool created successfully!\")\n",
    "print(\"📋 Tool description:\", retriever_tool.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🔧 Tool Features:**\n",
    "- **Similarity search**: Finds semantically relevant content\n",
    "- **Formatted output**: Returns structured, numbered results\n",
    "- **Error handling**: Graceful handling of empty results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Agent State and Architecture\n",
    "\n",
    "Define the agent's state and workflow structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Agent state schema defined\n"
     ]
    }
   ],
   "source": [
    "# Define the agent's state schema\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    The state of our agent conversation.\n",
    "    Messages are accumulated using the add_messages function.\n",
    "    \"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "print(\"📋 Agent state schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Available tools: ['retriever_tool']\n",
      "🤖 LLM bound with 1 tools\n"
     ]
    }
   ],
   "source": [
    "# Create tools list and bind to LLM\n",
    "tools = [retriever_tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Create tools dictionary for easy lookup\n",
    "tools_dict = {tool.name: tool for tool in tools}\n",
    "\n",
    "print(f\"🔧 Available tools: {list(tools_dict.keys())}\")\n",
    "print(f\"🤖 LLM bound with {len(tools)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🏗️ Architecture Components:**\n",
    "- **AgentState**: Maintains conversation history\n",
    "- **Tool binding**: Allows LLM to call our retrieval function\n",
    "- **Tools dictionary**: Enables dynamic tool execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Agent Functions \n",
    "\n",
    "Define the core agent functions for reasoning and tool execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Continuation logic defined\n"
     ]
    }
   ],
   "source": [
    "def should_continue(state: AgentState) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if the agent should continue with tool calls.\n",
    "    Returns True if the last message contains tool calls, False otherwise.\n",
    "    \"\"\"\n",
    "    last_message = state['messages'][-1]\n",
    "    has_tool_calls = hasattr(last_message, 'tool_calls') and len(last_message.tool_calls) > 0\n",
    "    \n",
    "    print(f\"🤔 Should continue? {has_tool_calls}\")\n",
    "    return has_tool_calls\n",
    "\n",
    "print(\"✅ Continuation logic defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📜 System prompt configured\n"
     ]
    }
   ],
   "source": [
    "# System prompt for the agent\n",
    "system_prompt = \"\"\"\n",
    "You are an intelligent AI assistant specialized in analyzing Stock Market Performance data from 2024.\n",
    "\n",
    "Your capabilities:\n",
    "- Use the retriever tool to search through the Stock Market Performance 2024 document\n",
    "- Provide accurate, data-driven answers based on the retrieved information\n",
    "- Make multiple tool calls if needed to gather comprehensive information\n",
    "- Always cite specific parts of the documents you reference\n",
    "\n",
    "Instructions:\n",
    "- When answering questions, first search for relevant information using the retriever tool\n",
    "- If you need additional context, make follow-up searches with different keywords\n",
    "- Always provide specific citations from the documents\n",
    "- Be clear about the source of your information\n",
    "\"\"\"\n",
    "\n",
    "print(\"📜 System prompt configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM function defined\n"
     ]
    }
   ],
   "source": [
    "def call_llm(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    The main reasoning function that calls the LLM with current state.\n",
    "    \"\"\"\n",
    "    messages = list(state['messages'])\n",
    "    # Add system prompt to the beginning\n",
    "    messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    \n",
    "    print(\"🧠 Calling LLM for reasoning...\")\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    print(f\"💬 LLM response type: {type(response).__name__}\")\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        print(f\"🔧 LLM wants to use {len(response.tool_calls)} tools\")\n",
    "    \n",
    "    return {'messages': [response]}\n",
    "\n",
    "print(\"✅ LLM function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tool execution function defined\n"
     ]
    }
   ],
   "source": [
    "def take_action(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Executes tool calls from the LLM's response.\n",
    "    \"\"\"\n",
    "    last_message = state['messages'][-1]\n",
    "    tool_calls = last_message.tool_calls\n",
    "    \n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call['name']\n",
    "        tool_args = tool_call['args']\n",
    "        \n",
    "        print(f\"🔧 Executing tool: {tool_name}\")\n",
    "        print(f\"📝 Query: {tool_args.get('query', 'No query provided')}\")\n",
    "        \n",
    "        if tool_name not in tools_dict:\n",
    "            print(f\"❌ Tool '{tool_name}' not found!\")\n",
    "            result = f\"Error: Tool '{tool_name}' does not exist.\"\n",
    "        else:\n",
    "            try:\n",
    "                result = tools_dict[tool_name].invoke(tool_args.get('query', ''))\n",
    "                print(f\"✅ Tool executed successfully. Result length: {len(str(result))} characters\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Tool execution error: {e}\")\n",
    "                result = f\"Error executing tool: {e}\"\n",
    "        \n",
    "        # Create tool message\n",
    "        tool_message = ToolMessage(\n",
    "            tool_call_id=tool_call['id'], \n",
    "            name=tool_name, \n",
    "            content=str(result)\n",
    "        )\n",
    "        results.append(tool_message)\n",
    "\n",
    "    print(f\"🔄 Returning {len(results)} tool results to LLM\")\n",
    "    return {'messages': results}\n",
    "\n",
    "print(\"✅ Tool execution function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🔄 Agent Workflow:**\n",
    "1. **call_llm**: LLM reasons and decides on actions\n",
    "2. **should_continue**: Checks if tools need to be called  \n",
    "3. **take_action**: Executes the chosen tools\n",
    "4. **Loop back**: Returns to LLM with tool results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Graph Construction\n",
    "\n",
    "Build the LangGraph workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Graph nodes added: llm, retriever_agent\n"
     ]
    }
   ],
   "source": [
    "# Create the state graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_node(\"llm\", call_llm)\n",
    "graph.add_node(\"retriever_agent\", take_action)\n",
    "\n",
    "print(\"📊 Graph nodes added: llm, retriever_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Graph edges and entry point configured\n"
     ]
    }
   ],
   "source": [
    "# Add conditional edges\n",
    "graph.add_conditional_edges(\n",
    "    \"llm\",                    # Start from LLM\n",
    "    should_continue,          # Decision function\n",
    "    {True: \"retriever_agent\", False: END}  # If True -> tools, if False -> end\n",
    ")\n",
    "\n",
    "# Add edge from tools back to LLM\n",
    "graph.add_edge(\"retriever_agent\", \"llm\")\n",
    "\n",
    "# Set the entry point\n",
    "graph.set_entry_point(\"llm\")\n",
    "\n",
    "print(\"🔗 Graph edges and entry point configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG Agent compiled successfully!\n",
      "🎯 Agent is ready to handle queries!\n"
     ]
    }
   ],
   "source": [
    "# Compile the graph\n",
    "rag_agent = graph.compile()\n",
    "\n",
    "print(\"✅ RAG Agent compiled successfully!\")\n",
    "print(\"🎯 Agent is ready to handle queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🎯 Graph Flow:**\n",
    "```\n",
    "Start → LLM → Decision → [Tools] → LLM → End\n",
    "                ↓         ↑\n",
    "                End   Results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Testing the Agent\n",
    "\n",
    "Let's test our RAG agent with a sample query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing Query: 'What were the key trends in the stock market during 2024?'\n",
      "==================================================\n",
      "🧠 Calling LLM for reasoning...\n",
      "💬 LLM response type: AIMessage\n",
      "🔧 LLM wants to use 1 tools\n",
      "🤔 Should continue? True\n",
      "🔧 Executing tool: retriever_tool\n",
      "📝 Query: key trends in stock market 2024\n",
      "✅ Tool executed successfully. Result length: 4478 characters\n",
      "🔄 Returning 1 tool results to LLM\n",
      "🧠 Calling LLM for reasoning...\n",
      "💬 LLM response type: AIMessage\n",
      "🤔 Should continue? False\n",
      "\n",
      "🤖 Agent Response:\n",
      "In 2024, the stock market was characterized by several key trends:\n",
      "\n",
      "1. **Tech-Dominated Rally**: The year was marked by a significant rally in technology stocks, which delivered substantial wealth gains. This rally was driven by strong growth narratives tied to transformational tech trends such as artificial intelligence, cloud computing, and quantum technology. The market saw exceptional performances from mega-cap tech companies like Apple, Alphabet, and Meta, as well as newer companies riding the wave of tech innovation [Document 1, Document 3].\n",
      "\n",
      "2. **Strong Market Performance**: The S&P 500 index delivered a roughly 25% total return for the year, marking the second consecutive year of over 20% returns. The tech-heavy Nasdaq Composite outpaced the broader market with a nearly 29% increase. However, smaller-cap stocks had more modest performance, with indices like the S&P 500 Equal-Weight index and the Russell 2000 rising about 10-11% [Document 5].\n",
      "\n",
      "3. **Concentration of Market Leadership**: The rally was not evenly distributed across the market, with a significant concentration of gains in a few mega-cap technology stocks, often referred to as the \"Magnificent 7\" (Apple, Microsoft, Alphabet, Amazon, Meta, etc.) [Document 5].\n",
      "\n",
      "4. **Increased Valuations**: The surge in stock prices led to significantly increased valuations, with many leading stocks trading at rich multiples by year-end. This was driven by optimism for future growth, although it also raised caution among investors about potential corrections [Document 3].\n",
      "\n",
      "5. **Economic Resilience and AI Adoption**: The trends propelling the gains included economic resilience and the adoption of AI, which were expected to continue into 2025. However, there was also caution about potential corrections due to the rapid rise in stock prices [Document 1].\n",
      "\n",
      "Overall, 2024 was a banner year for stocks, led overwhelmingly by the technology sector, with major indices reaching record or near-record highs [Document 3].\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def test_agent_query(question: str):\n",
    "    \"\"\"\n",
    "    Test function to demonstrate agent capabilities\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Testing Query: '{question}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create human message\n",
    "    messages = [HumanMessage(content=question)]\n",
    "    \n",
    "    # Run the agent\n",
    "    result = rag_agent.invoke({\"messages\": messages})\n",
    "    \n",
    "    # Print the final response\n",
    "    final_response = result['messages'][-1].content\n",
    "    print(f\"\\n🤖 Agent Response:\\n{final_response}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with a sample question\n",
    "sample_question = \"What were the key trends in the stock market during 2024?\"\n",
    "test_result = test_agent_query(sample_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🧪 What to expect:**\n",
    "- The agent will search the document for relevant information\n",
    "- It will provide a comprehensive answer with citations\n",
    "- You'll see the tool execution logs in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Interactive Agent Interface \n",
    "\n",
    "Create an interactive interface to chat with your agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipywidgets) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
      "Requirement already satisfied: wcwidth in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241cd0ab805a4df3b1e22b4a1da8434e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>🤖 RAG Agent - Stock Market Assistant</h3>'), Textarea(value='', description='Qu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    def create_interactive_widget():\n",
    "        \"\"\"\n",
    "        Creates an interactive widget interface for the RAG agent\n",
    "        Requires: pip install ipywidgets\n",
    "        \"\"\"\n",
    "        # Create widgets\n",
    "        question_input = widgets.Textarea(\n",
    "            placeholder=\"Enter your question about stock market performance...\",\n",
    "            description=\"Question:\",\n",
    "            layout=widgets.Layout(width='70%', height='80px')\n",
    "        )\n",
    "        \n",
    "        ask_button = widgets.Button(\n",
    "            description=\"Ask Agent\",\n",
    "            button_style='primary',\n",
    "            icon='search'\n",
    "        )\n",
    "        \n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        def on_ask_button_click(b):\n",
    "            with output_area:\n",
    "                clear_output(wait=True)\n",
    "                if question_input.value.strip():\n",
    "                    print(f\"🔍 Question: {question_input.value}\")\n",
    "                    print(\"=\" * 50)\n",
    "                    print(\"🔄 Processing...\")\n",
    "                    \n",
    "                    try:\n",
    "                        messages = [HumanMessage(content=question_input.value)]\n",
    "                        result = rag_agent.invoke({\"messages\": messages})\n",
    "                        \n",
    "                        print(\"\\n🤖 AGENT RESPONSE:\")\n",
    "                        print(\"-\" * 30)\n",
    "                        print(result['messages'][-1].content)\n",
    "                        print(\"=\" * 50)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error: {e}\")\n",
    "                else:\n",
    "                    print(\"⚠️ Please enter a question!\")\n",
    "        \n",
    "        ask_button.on_click(on_ask_button_click)\n",
    "        \n",
    "        # Display the interface\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h3>🤖 RAG Agent - Stock Market Assistant</h3>\"),\n",
    "            question_input,\n",
    "            ask_button,\n",
    "            output_area\n",
    "        ]))\n",
    "    \n",
    "    # Uncomment to create the widget interface:\n",
    "    create_interactive_widget()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"ipywidgets not available. Use the simple ask_agent() function instead.\")\n",
    "    \n",
    "# Example question: What were the key stock market trends in 2024? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**💡 Usage Tips:**\n",
    "\n",
    "- Ask specific questions about stock market data\n",
    "- Request comparisons between different time periods\n",
    "- Ask for trends, analysis, or specific metrics\n",
    "- The agent will search and cite relevant document sections\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Advanced Features\n",
    "**Multiple Document Support**\n",
    "To extend this agent for multiple documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: Stock_Market_Performance_2024.pdf (9 pages)\n",
      "✅ Loaded: U.S._Economic_Outlook.pdf (29 pages)\n"
     ]
    }
   ],
   "source": [
    "def load_multiple_documents(pdf_paths: list):\n",
    "    \"\"\"\n",
    "    Example function to load multiple PDF documents\n",
    "    \"\"\"\n",
    "    all_pages = []\n",
    "    \n",
    "    for pdf_path in pdf_paths:\n",
    "        if os.path.exists(pdf_path):\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            pages = loader.load()\n",
    "            all_pages.extend(pages)\n",
    "            print(f\"✅ Loaded: {pdf_path} ({len(pages)} pages)\")\n",
    "        else:\n",
    "            print(f\"⚠️ File not found: {pdf_path}\")\n",
    "    \n",
    "    return all_pages\n",
    "\n",
    "# Example usage (uncomment and modify paths as needed):\n",
    "document_paths = [\n",
    "    \"Stock_Market_Performance_2024.pdf\",\n",
    "    \"U.S._Economic_Outlook.pdf\"\n",
    "]\n",
    "all_documents = load_multiple_documents(document_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enhanced Search Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_retriever(vectorstore, search_type=\"mmr\"):\n",
    "    \"\"\"\n",
    "    Create retriever with Maximum Marginal Relevance for diverse results\n",
    "    \"\"\"\n",
    "    return vectorstore.as_retriever(\n",
    "        search_type=search_type,  # \"mmr\" for diverse results\n",
    "        search_kwargs={\n",
    "            \"k\": 6,              # Return more results\n",
    "            \"fetch_k\": 20,       # Consider more candidates\n",
    "            \"lambda_mult\": 0.7   # Balance relevance vs diversity\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Example: Enhanced retriever (uncomment to use)\n",
    "enhanced_retriever = create_advanced_retriever(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Key Concepts Summary\n",
    "**🎯 What We Built**\n",
    "\n",
    "- Document Processing Pipeline: PDF → Text Chunks → Embeddings → Vector Store\n",
    "- Retrieval System: Semantic search for relevant information\n",
    "- Agent Architecture: LLM + Tools + State Management\n",
    "- Interactive Interface: User-friendly query system\n",
    "\n",
    "**🔑 Key Components**\n",
    "| Component | Purpose | Technology |\n",
    "|-----------|---------|------------|\n",
    "| **PDF Loader** | Extract text from documents | PyPDFLoader |\n",
    "| **Text Splitter** | Create manageable chunks | RecursiveCharacterTextSplitter |\n",
    "| **Embeddings** | Convert text to vectors | OpenAI text-embedding-3-small |\n",
    "| **Vector Store** | Store and search embeddings | ChromaDB |\n",
    "| **LLM** | Reasoning and response generation | GPT-4o |\n",
    "| **Agent Framework** | Orchestrate workflow | LangGraph |\n",
    "\n",
    "**🚀 Capabilities Achieved**\n",
    "\n",
    "- ✅ Accurate Information Retrieval: Finds relevant document sections\n",
    "- ✅ Contextual Understanding: Maintains conversation context\n",
    "- ✅ Multi-step Reasoning: Can make multiple searches if needed\n",
    "- ✅ Source Citation: References specific document parts\n",
    "- ✅ Interactive Interface: User-friendly question-answering\n",
    "\n",
    "**🔮 Next Steps**\n",
    "\n",
    "- Add More Tools: Web search, calculator, database queries, voice agent\n",
    "- Improve Chunking: Experiment with different strategies\n",
    "- Multi-modal Support: Add image and table processing\n",
    "- Evaluation Metrics: Implement retrieval and response quality metrics\n",
    "- Production Deployment: Add error handling, logging, and monitoring\n",
    "\n",
    "**📚 Further Learning**\n",
    "\n",
    "- LangChain Documentation: langchain.com\n",
    "- LangGraph Tutorials: langgraph.com\n",
    "- RAG Best Practices: Advanced chunking and retrieval strategies\n",
    "- Agent Design Patterns: Multi-agent systems and tool composition\n",
    "\n",
    "\n",
    "🎉 Congratulations! You've successfully built a complete RAG agent that can intelligently search through documents and provide informed responses. This foundation can be extended to handle multiple documents, different file types, and more sophisticated reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
