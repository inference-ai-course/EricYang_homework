{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Week 5: Supervised Finetuning (SFT) - I\n",
        "**Theme:** Teaching LLMs to Follow Instructions  \n",
        "**Project:** LoRA vs. Full Finetuning on HuggingFace with Deepspeed/TRL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìò 1. What is Supervised Finetuning (SFT)?\n",
        "SFT = Pretrained model + Instruction-following data ‚Üí Task-specific model\n",
        "\n",
        "**Supervised Fine-Tuning (SFT)** is the process of further training a pre-trained language model on a labeled dataset to specialize it for specific tasks or domains.\n",
        "\n",
        "**Key points:**\n",
        "- Builds on top of a pretrained model like LLaMA, GPT, or Mistral.\n",
        "- Uses instruction-response pairs (like question-answer).\n",
        "- Enhances instruction-following ability.\n",
        "- It's a middle stage between pretraining and alignment (e.g., RLHF).\n",
        "\n",
        "**SFT Pipeline:**\n",
        "1. Pretrained Model\n",
        "2. Supervised Dataset\n",
        "3. Fine-tuned Instruction Model\n",
        "\n",
        "\n",
        "**Example:**\n",
        "| Before SFT                 | After SFT                          |\n",
        "|---------------------------|------------------------------------|\n",
        "| Random generic responses  | Follows user instructions clearly |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 2. How to Get SFT Data\n",
        "\n",
        "**4 Types of Data Sources:**\n",
        "1. **Manual Curation**: Human-created prompts and responses.\n",
        "2. **AI-Generated**: Use GPT models to self-generate instruction data.\n",
        "3. **Open Datasets**: Alpaca, OASST1, Dolly, HH-RLHF, etc.\n",
        "4. **Data Augmentation**: Rephrasing, adding context, changing perspective.\n",
        "\n",
        "**Goal**: Create high-quality, diverse, and instruction-aligned examples.\n",
        "\n",
        "* here we use the second way to generate our data using openAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'question': 'Can you explain the difference between synchronous and asynchronous programming?',\n",
              "  'answer': 'Synchronous programming executes tasks one after the other, meaning each task must complete before the next one starts. In contrast, asynchronous programming allows tasks to run concurrently, enabling the program to initiate a task and move on to the next one without waiting for the previous task to finish. This is particularly useful in scenarios such as web development where a user interface needs to remain responsive while waiting for data to load.'},\n",
              " {'question': 'What are some common design patterns you have used in your projects?',\n",
              "  'answer': \"Some common design patterns I have used include the Singleton pattern, which ensures a class has only one instance and provides a global point of access to it, and the Observer pattern, which defines a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. I've also used the Factory pattern to create objects without specifying the exact class of the object that will be created.\"}]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def get_ai_generated_data():\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"‚ö†Ô∏è No OpenAI API key - using placeholder data\")\n",
        "        return [{\"instruction\": \"What are your technical skills?\", \"response\": \"Python, data analysis\"}]\n",
        "\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    prompt = \"Create 2 interview Q&A pairs for a software developer in JSON format. Output only JSON.\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # If model returns Markdown-style JSON block\n",
        "    if \"```json\" in content:\n",
        "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
        "    elif \"```\" in content:\n",
        "        content = content.split(\"```\")[1]\n",
        "\n",
        "    try:\n",
        "        data = json.loads(content)\n",
        "        if isinstance(data, dict):\n",
        "            return data.get(\"examples\", [])\n",
        "        elif isinstance(data, list):\n",
        "            return data\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Unexpected JSON format:\", type(data))\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to parse JSON:\", e)\n",
        "        print(\"Raw content:\", content)\n",
        "        return []\n",
        "\n",
        "# Example call\n",
        "get_ai_generated_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß© 3. Formatting: ChatML\n",
        "**ChatML** is a structured dialogue format used to simulate role-based conversations during SFT training.\n",
        "\n",
        "**Structure:**\n",
        "```\n",
        "<|im_start|>user\n",
        "What's the capital of France?\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Paris.\n",
        "<|im_end|>\n",
        "```\n",
        "\n",
        "**Why it matters:**\n",
        "- Improves consistency\n",
        "- Helps multi-turn dialogue modeling\n",
        "- Matches formatting expectations for LLaMA and OpenAI-style models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç 4. Full Finetune vs. LoRA\n",
        "\n",
        "\n",
        "| Aspect               | Full Fine-Tuning      | LoRA (Low-Rank Adaptation) |\n",
        "|----------------------|-----------------------|-----------------------------|\n",
        "| Trainable Params     | 100%                  | ~0.5‚Äì1%                     |\n",
        "| Memory Usage         | Very High             | Low                         |\n",
        "| Flexibility          | Maximum               | Good for most tasks         |\n",
        "| Training Time        | Longer                | Faster                      |\n",
        "| Use Case             | Critical domain shift | Resource-efficient tuning   |\n",
        "\n",
        "**Recommendation**: Use LoRA for most educational and practical settings unless full retraining is justified.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-28 15:19:19,972] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to mps (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0528 15:19:20.075000 66105 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
            "trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
            "/Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/.venv/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "model_name = \"microsoft/DialoGPT-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"]\n",
        ")\n",
        "lora_model = get_peft_model(base_model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° 5. DeepSpeed\n",
        "\n",
        "**DeepSpeed** is a library from Microsoft that allows efficient distributed training of large models.\n",
        "\n",
        "**Modes:**\n",
        "- **ZeRO-1/2/3** for optimizer/shard parallelism.\n",
        "- **CPU Offload** to reduce GPU memory usage.\n",
        "- **Mixed Precision** for speed and efficiency.\n",
        "\n",
        "**Best for**: Scaling training to large models like 13B+, saving memory, or training on multiple GPUs.\n",
        "\n",
        "Enables memory-efficient training. Example config:\n",
        "```json\n",
        "{\n",
        "  \"zero_optimization\": {\"stage\": 2},\n",
        "  \"fp16\": {\"enabled\": true}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è 6. TRL Package (SFTTrainer)\n",
        "\n",
        "**Transformers Reinforcement Learning (TRL)** by Hugging Face includes:\n",
        "\n",
        "- `SFTTrainer`: Simplified supervised training loop.\n",
        "- `PPOTrainer`: RLHF with Proximal Policy Optimization.\n",
        "- `DPOTrainer`: Direct Preference Optimization.\n",
        "- `RewardTrainer`: For reward model training.\n",
        "\n",
        "**Why TRL?**\n",
        "- Abstracts away complex setup.\n",
        "- Faster experimentation.\n",
        "- Supports all major fine-tuning and alignment workflows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting train dataset to ChatML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 496.66 examples/s]\n",
            "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1088.72 examples/s]\n",
            "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 568.80 examples/s]\n",
            "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1132.52 examples/s]\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>9.589100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>9.275600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=9.432311534881592, metrics={'train_runtime': 2.921, 'train_samples_per_second': 0.685, 'train_steps_per_second': 0.685, 'total_flos': 18722451456.0, 'train_loss': 9.432311534881592})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "\n",
        "# Prepare the dataset (must have a 'text' field)\n",
        "demo_dataset = Dataset.from_list([\n",
        "    {\"text\": \"Human: What is Python?\\nAssistant: Python is a programming language.\"},\n",
        "    {\"text\": \"Human: How do I learn coding?\\nAssistant: Start with basic concepts and practice regularly.\"}\n",
        "])\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./trl_sft_demo\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=5e-4,\n",
        "    logging_steps=1,\n",
        "    save_steps=10,\n",
        "    save_total_limit=1,\n",
        "    fp16=False,\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "# ‚úÖ Initialize SFTTrainer ‚Äî no config, no extras\n",
        "trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=demo_dataset\n",
        ")\n",
        "\n",
        "# ‚úÖ Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a99c70d",
      "metadata": {},
      "source": [
        "Explanation:\n",
        "1. global_step=2\n",
        "\n",
        "    This means the training process completed 2 optimization steps (i.e., two batches were processed and used to update the model's parameters).\n",
        "\n",
        "2. training_loss=9.432311534881592\n",
        "\n",
        "    This is the final training loss averaged over the training steps. A higher loss suggests the model hasn't learned much yet, likely because:\n",
        "        * It‚Äôs early in training (only 2 steps).\n",
        "        * The model needs more tuning or a better learning rate.\n",
        "        * The data might be complex or noisy.\n",
        "\n",
        "3. train_runtime=2.921\n",
        "\n",
        "    Total time in seconds the training took ‚Äî in this case, around 2.9 seconds.\n",
        "\n",
        "4. train_samples_per_second=0.685\n",
        "\n",
        "    The average number of training examples processed per second. Since only 2 steps were taken, the dataset or batch size may have been small.\n",
        "\n",
        "5. train_steps_per_second=0.685\n",
        "\n",
        "    How many steps (i.e., parameter updates) were completed per second. It matches the sample rate, implying 1 sample per step.\n",
        "\n",
        "6. total_flos=18722451456.0\n",
        "\n",
        "    The total number of floating point operations (FLOPs) executed during training. It's a proxy for how computationally intensive the training was.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f662325",
      "metadata": {},
      "source": [
        "### Explanation of Each Metric during model training:\n",
        "- ‚úÖ loss\n",
        "    *What it is*: Measures the model's prediction error ‚Äî how far off the model is from the target output.\n",
        "\n",
        "    *What to look for*: We want this to decrease over time.\n",
        "\n",
        "    A value of 0.1097 or 0.1366 is relatively low, which is promising, assuming it continues trending downward.\n",
        "\n",
        "    Temporary small increases (like from 0.1097 to 0.1366) can happen due to learning rate fluctuations or noisy batches.\n",
        "\n",
        "- ‚úÖ grad_norm (Gradient Norm)\n",
        "    *What it is*: L2 norm of the gradients ‚Äî essentially, how large the updates to the model's weights are.\n",
        "\n",
        "    *What to look for*:\n",
        "\n",
        "    If this value is too large, it may indicate exploding gradients.\n",
        "\n",
        "    If too small (near zero), it may mean vanishing gradients or that training is plateauing.\n",
        "\n",
        "    for example:\n",
        "\n",
        "    0.969 ‚Üí healthy magnitude, meaning the model is still learning.\n",
        "\n",
        "    0.278 ‚Üí much smaller, which could mean learning is slowing down ‚Äî possibly nearing convergence, or may need LR adjustment.\n",
        "\n",
        "- ‚úÖ learning_rate\n",
        "    *What it is*: The rate at which the model updates its weights. Often decays over time (e.g., cosine scheduler).\n",
        "\n",
        "    *What to look for*:\n",
        "\n",
        "    A decaying learning rate is common and helps fine-tune the model toward convergence.\n",
        "\n",
        "    0.000126 ‚Üí slightly higher; 0.000120 ‚Üí lower. This drop suggests a learning rate schedule is being applied, as expected.\n",
        "\n",
        "- ‚úÖ epoch\n",
        "    *What it is*: Indicates how far along you are in training (e.g., 4.67 = 67% through the 5th epoch).\n",
        "\n",
        "    *What to look for*: Helps track progression. You‚Äôd want to compare loss and grad_norm across epochs to evaluate learning trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb59b478",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4094f035",
      "metadata": {},
      "source": [
        "## ‚úÖ Summary\n",
        "\n",
        "You‚Äôve learned:\n",
        "- What SFT is and why it‚Äôs essential\n",
        "- Where and how to get quality data\n",
        "- How to use ChatML format\n",
        "- When to choose LoRA vs full tuning\n",
        "- How to leverage DeepSpeed and TRL for scale and alignment\n",
        "\n",
        "## - For the full llama3 sft code, check out class_5_llama3.py"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
