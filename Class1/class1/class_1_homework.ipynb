{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MCP Homework and Local LLM Implementation with Ollama & LangChain**\n",
    "*In this thrust we have four tasks, which will be enhancing MCP using and learning how to use two brilliant tools for LLM application.*\n",
    "\n",
    "Prework:  follow the instruction in slides.Implement the MCP in Local Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Using MCP to built Agent-like work-flow.\n",
    "\n",
    "**Task 1.1 MCP + Claude = Browser Automation**\\\n",
    "*use MCP + Claude to do browser automation tasks*\\\n",
    "\\\n",
    "\\\n",
    "step 1: Learn all kinds of cool MCP server using awesome-mcp-servers\\\n",
    "[Introducing to more MCP server](https://github.com/punkpeye/awesome-mcp-servers)\n",
    "\n",
    "step 2: Install cline\\\n",
    "[cline](https://cline.bot/)\n",
    "\n",
    "step 3: using playwright, follow the instruction to built a browser automation workflow\\\n",
    "<img src=\"https://www.automatalabs.io/_next/static/media/icon.7c0ac385.svg\" alt=\"jupyter\" width=\"200\"/>\\\n",
    "[Playwright](https://github.com/Automata-Labs-team/MCP-Server-Playwright)\n",
    "\n",
    "step 4: Open your Claude and use playwright to ask the model to login your github account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Play with Ollama\n",
    "\n",
    "<img src=\"https://ollama.com/public/blog/meta-ollama-llama3.png\" alt=\"jupyter\" width=\"500\"/>\n",
    "\n",
    "***Ollama** is a **convenient** and **free** framework，designed for easy deployment and running of large language models (LLMs) locally.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 2.1: Install Ollama and run LLMs locally**  \n",
    "- refer to [Ollama](https://ollama.ai/) to complete installation.  \n",
    "- Run `ollama run llama2` from the command line to download and launch the `llama2` model.\n",
    "\n",
    "\n",
    "**Task 2.2: Using Ollama to call OpenAI API**\\\n",
    "*Ollama now has built-in compatibility with the OpenAI Chat Completions API, making it possible to use more tooling and applications with Ollama locally.*\n",
    "\n",
    "See official instruction below：\\\n",
    "[Ollama OpenAI Compatibility](https://ollama.com/blog/openai-compatibility)\\\n",
    "[Ollama OpenAI](https://github.com/ollama/ollama/blob/main/docs/openai.md)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To invoke Ollama’s OpenAI compatible API endpoint, use the same OpenAI format and change the hostname to http://localhost:11434:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curl Method:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curl http://localhost:11434/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"llama2\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hello!\"\n",
    "            }\n",
    "        ]\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama2\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In above examples, “Ollama” is essentially acting as a local server that is compatible with the OpenAI API. In other words, the endpoint you’re calling—whether via code or a cURL command—is not the official OpenAI endpoint at https://api.openai.com/v1/ but rather http://localhost:11434/v1/. The local process running on this port is Ollama.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Combining LangChain with Ollama’s local LLMs\n",
    "\n",
    "*LangChain simplifies every stage of the LLM application lifecycle.\n",
    "It unifies functional modules such as \"Prompt design\", \"Multi-round dialogue memory (Memory)\", \"External data retrieval (Retrieval)\" and \"Tools/Agents\" into a unified package. In this way, you do not need to manually manage each step of the language model call and data flow, and only need to focus on business logic.*\n",
    "\n",
    "LangChain–Ollama Documentation: [https://python.langchain.com/docs/integrations/llms/ollama](https://python.langchain.com/docs/integrations/llms/ollama)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*-PlFCd_VBcALKReO3ZaOEg.png\" alt=\"jupyter\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1 Reproduce practice in lecture using LCEL**\n",
    "\n",
    "*A chain is a sequence of steps or model calls connected together to achieve a larger task. Each step can involve retrieving information, transforming text, or invoking a language model in some way, and then passing its output on to the next step in the chain. This structure helps you build more complex workflows or pipelines using multiple actions in a simple, organized manner.*\n",
    "\n",
    "- what is LCEL? LCEL is a much simpler way to construct \"Chain\"\\\n",
    "[LCEL](https://python.langchain.com/docs/concepts/lcel/)\\\n",
    "why use it?\\\n",
    "[Why LCEL](https://python.langchain.com/v0.1/docs/expression_language/why/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code with a Ollama local model："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using LCEL to reproduce a \"Basic Prompting\" scenario\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama \n",
    "\n",
    "# 2. Define the prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is the capital of {topic}?\"\n",
    ")\n",
    "\n",
    "# 3. Define the model\n",
    "model = ChatOllama(model = [\"llama2\"])  # Using Ollama \n",
    "\n",
    "# 4. Chain the components together using LCEL\n",
    "chain = (\n",
    "    # LCEL syntax: use the pipe operator | to connect each step\n",
    "    {\"topic\": RunnablePassthrough()}  # Accept user input\n",
    "    | prompt                          # Transform it into a prompt message\n",
    "    | model                           # Call the model\n",
    "    | StrOutputParser()               # Parse the output as a string\n",
    ")\n",
    "\n",
    "# 5. Execute\n",
    "result = chain.invoke(\"Germany\")\n",
    "print(\"User prompt: 'What is the capital of Germany?'\")\n",
    "print(\"Model answer:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Submission Requirements**  \n",
    "1. Complete all tasks.  \n",
    "2. Include screenshots of key outputs (e.g., model responses, agent computation results).\n",
    "\n",
    "- Advance Work：Integrate the Ollama and Langchain tasks into **Gradio Web UI**, which will be useful for building Proxy AI-Agent interface translation with front-end, and demonstrate your work.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
