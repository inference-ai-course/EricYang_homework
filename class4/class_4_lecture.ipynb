{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Retrieval-Augmented Generation (RAG) â€“ Python Class\n",
    "## Learning Objectives\n",
    "* Understand the RAG architecture and its components\n",
    "\n",
    "* Implement text embedding and similarity search\n",
    "\n",
    "* Process text through tokenization, filtering, and chunking\n",
    "\n",
    "* Store and retrieve data using a vector database\n",
    "\n",
    "* Integrate RAG with LLMs using LangChain\n",
    "\n",
    "* Evaluate RAG system performance using appropriate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "Ensure you have the following packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade langchain langchain-core langchain-community langchain-openai langchain-experimental chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting your OpenAI API key as an environment variable in .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in your .env file, change your-openai-api-key to actual openai api key\n",
    "OPENAI_API_KEY='your-openai-api-key'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Breakdown of the RAG Pipeline\n",
    "### 1. Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Loads variables from a .env file into the environment\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is a powerful framework that facilitates the development of applications powered by large language models (LLMs).  <br/>\n",
    "When integrating OpenAI's models (like GPT-3.5 or GPT-4) into your LangChain application, authentication is essential.  <br/>\n",
    "This is achieved through the OPENAI_API_KEY, which grants authorized access to OpenAI's API endpoints. <br/>\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a `.env` File:\n",
    "In the root directory of your project, create a file named `.env`.\n",
    "\n",
    "2. Add Your API Key:\n",
    "Inside the `.env`file, add the following line:\n",
    "`OPENAI_API_KEY`=`your_openai_api_key_here`\n",
    "\n",
    "This code snippet reads the .env file and sets the `OPENAI_API_KEY` in your environment, making it accessible to your application.\n",
    "\n",
    "### How LangChain Utilizes the API Key\n",
    "Once the `OPENAI_API_KEY` is set in your environment, LangChain's OpenAI integrations can seamlessly access it.\n",
    " For instance, when you initialize a language model in LangChain:\n",
    "\n",
    "        \n",
    "```python\n",
    "                        from langchain_openai import OpenAI\n",
    "                        llm = OpenAI()\n",
    "```\n",
    "LangChain internally retrieves the API key from the environment variable and uses it to authenticate requests to OpenAI's API. <br/>\n",
    "This design abstracts away the need to manually pass the API key each time, promoting cleaner and more secure code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize the RAG Class with Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.RAGClass object at 0x132170f70>\n"
     ]
    }
   ],
   "source": [
    "rag = RAGClass(data_path=\"my_text_file.txt\")\n",
    "print(rag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Here, you instantiate the RAGClass, providing the path to your text data. This class will manage the RAG pipeline processes.\n",
    "\n",
    "Expected Outcome:\n",
    "An instance of RAGClass is created, ready to process the specified text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Document 1 content preview:</b><br>Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external ...<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my_text_file.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources.\\n\\nText embeddings are numerical representations of text that capture semantic meaning. They are used to measure similarity between pieces of text.\\n\\nVector databases store these embeddings and allow for efficient similarity searches.\\n\\nLangChain is a framework that facilitates the development of applications powered by language models. It provides tools for integrating LLMs with external data sources.\\n\\nEvaluation metrics for RAG systems include precision, recall, and F1 score, which assess the quality of the generated responses.\\n')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "This method uses LangChain's TextLoader to read the contents of the specified text file and load them into the pipeline.\n",
    "\n",
    "Expected Outcome:\n",
    "The text file is read, and its contents are stored as a document within the RAGClass instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split Documents into Chunks\n",
    "\n",
    "```python\n",
    "\n",
    "    def split_documents(self, chunk_size=500, chunk_overlap=50):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        self.text_chunks = text_splitter.split_documents(self.documents)\n",
    "        print(f\"Split documents into {len(self.text_chunks)} chunks.\")\n",
    "        \n",
    "        # Print each chunk with line breaks after periods\n",
    "        for i, chunk in enumerate(self.text_chunks):\n",
    "            print(f\"\\nChunk {i+1}:\")\n",
    "            formatted_text = chunk.page_content.replace(\". \", \".\\n\")\n",
    "            print(formatted_text)\n",
    "\n",
    "```\n",
    "\n",
    "<details>\n",
    "  <summary>Code explaination</summary>\n",
    "  <text>\n",
    "* Understanding RecursiveCharacterTextSplitter\n",
    "    The RecursiveCharacterTextSplitter is a utility in LangChain designed to divide large text documents into smaller, manageable chunks. </br>\n",
    "    This is particularly useful when working with language models that have input size limitations.\n",
    "\n",
    "* Key Parameters:\n",
    "\n",
    "    * chunk_size: Defines the maximum number of characters in each chunk.\n",
    "    * chunk_overlap: Specifies the number of characters that should overlap between consecutive chunks. This overlap ensures context is preserved across chunks.\n",
    "\n",
    "* How It Works\n",
    "The splitter operates by attempting to divide the text using a hierarchy of separators, aiming to preserve the semantic integrity of the content. The default separator hierarchy is:\n",
    "\n",
    "    1. Paragraph breaks (\\n\\n)\n",
    "    2. Line breaks (\\n)\n",
    "    3. DataStax Documentation\n",
    "    4. Spaces ( )\n",
    "    5. No separator (character-level split)\n",
    "\n",
    "    The splitter recursively applies these separators to break the text into chunks that do not exceed the specified chunk_size. </br>\n",
    "    If a chunk is still too large after applying all separators, it will be split at the character level.\n",
    "  </text>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split documents into 2 chunks.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk 1:</b><br>Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.<br>It enhances the capabilities of language models by providing them with access to external knowledge sources.\n",
       "\n",
       "Text embeddings are numerical representations of text that capture semantic meaning.<br>They are used to measure similarity between pieces of text.\n",
       "\n",
       "Vector databases store these embeddings and allow for efficient similarity searches.<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk 2:</b><br>LangChain is a framework that facilitates the development of applications powered by language models.<br>It provides tools for integrating LLMs with external data sources.\n",
       "\n",
       "Evaluation metrics for RAG systems include precision, recall, and F1 score, which assess the quality of the generated responses.<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my_text_file.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources.\\n\\nText embeddings are numerical representations of text that capture semantic meaning. They are used to measure similarity between pieces of text.\\n\\nVector databases store these embeddings and allow for efficient similarity searches.'),\n",
       " Document(metadata={'source': 'my_text_file.txt'}, page_content='LangChain is a framework that facilitates the development of applications powered by language models. It provides tools for integrating LLMs with external data sources.\\n\\nEvaluation metrics for RAG systems include precision, recall, and F1 score, which assess the quality of the generated responses.')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.split_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Vector Store\n",
    "\n",
    "```python\n",
    "    def create_vectorstore(self):\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        self.vectorstore = Chroma.from_documents(self.text_chunks, embedding=embeddings)\n",
    "```\n",
    "\n",
    "<details>\n",
    "  <summary>Code explaination</summary>\n",
    "  <text>\n",
    "* This function performs two primary tasks:\n",
    "\n",
    "Initialize the Embedding Model: It creates an instance of OpenAIEmbeddings, which is responsible for generating vector representations (embeddings) of text using OpenAI's embedding models.\n",
    "\n",
    "Create the Vector Store: It constructs a Chroma vector store from the previously split text chunks, utilizing the embeddings generated in the first step.\n",
    "\n",
    "1. Initializing OpenAIEmbeddings\n",
    "        `embeddings = OpenAIEmbeddings()`\n",
    "* Purpose: This line initializes the embedding model that will convert text into numerical vector representations. </br>\n",
    "By default, it uses OpenAI's text-embedding-ada-002 model, but you can specify other models if needed.\n",
    "\n",
    "* Interaction with OpenAI API:\n",
    "    * When you later use this embeddings instance to embed text (as in the next step), it sends requests to OpenAI's API, utilizing your provided API key for authentication.\n",
    "\n",
    "    * Each text chunk is sent to the API, and the corresponding embedding vector is returned. \n",
    "\n",
    "2. Creating the Chroma Vector Store\n",
    "    `self.vectorstore = Chroma.from_documents(self.text_chunks, embedding=embeddings)`\n",
    "\n",
    "* Purpose: This line creates a Chroma vector store by embedding each text chunk and storing the resulting vectors. Chroma is an open-source vector database optimized for handling embeddings and facilitating similarity searches.\n",
    "\n",
    "* Process:\n",
    "\n",
    "    * Embedding Text Chunks: Each document in `self.text_chunks` is passed through the embeddings instance to obtain its vector representation.\n",
    "\n",
    "    * Storing Embeddings: The resulting vectors are stored in the Chroma vector store, allowing for efficient similarity searches later on.\n",
    "\n",
    "* Interaction with OpenAI API:\n",
    "\n",
    "    * For each document, a request is sent to OpenAI's embedding endpoint to obtain its vector representation.\n",
    "\n",
    "    * These embeddings are then stored in Chroma, not in OpenAI's systems.\n",
    "  </text>\n",
    "</details>\n",
    "\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "  <summary>Integration in a RAG Pipeline</summary>\n",
    "  <text>\n",
    "\n",
    "In a Retrieval-Augmented Generation (RAG) pipeline, this function is crucial for setting up the vector store that enables efficient retrieval of relevant information:\n",
    "\n",
    "1. Document Processing: Raw documents are loaded and split into manageable chunks.\n",
    "\n",
    "2. Embedding Generation: Each chunk is converted into a vector using OpenAI's embedding models.\n",
    "\n",
    "3. Vector Store Creation: The embeddings are stored in Chroma, facilitating quick similarity searches.\n",
    "\n",
    "4. Query Handling: When a user query is received, it's embedded and compared against the stored vectors to retrieve the most relevant chunks.\n",
    "\n",
    "5. Answer Generation: The retrieved chunks provide context to a language model (like OpenAI's GPT-4) to generate a comprehensive answer.\n",
    "  </text>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with embedded documents.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Vectorstore Contents:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Document 1:</b><br>Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.<br>It enhances the capabilities of language models by providing them with access to external knowledge sources.\n",
       "\n",
       "Text embeddings are numerical representations of text that capture semantic meaning.<br>They are used to measure similarity between pieces of text.\n",
       "\n",
       "Vector databases store these embeddings and allow for efficient similarity searches.<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Document 2:</b><br>LangChain is a framework that facilitates the development of applications powered by language models.<br>It provides tools for integrating LLMs with external data sources.\n",
       "\n",
       "Evaluation metrics for RAG systems include precision, recall, and F1 score, which assess the quality of the generated responses.<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x133842920>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.create_vectorstore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Each text chunk is converted into a vector embedding using OpenAI's embedding model. These embeddings are stored in a vector database (e.g., Chroma) for similarity searches.\n",
    "\n",
    "Expected Outcome:\n",
    "A vector store is created, containing embeddings of all text chunks, enabling efficient similarity-based retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up Retriever\n",
    "```python \n",
    "    def setup_retriever(self):\n",
    "        \"\"\"\n",
    "        Sets up a retriever from the vectorstore for similarity search.\n",
    "        Returns the retriever object.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\"Vectorstore not initialized.\")\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "        print(\"Retriever set up from vectorstore.\")\n",
    "        display(HTML(f\"<b>Retriever details:</b> {self.retriever}\"))\n",
    "        return self.retriever\n",
    "```\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Code explaination</summary>\n",
    "  <text>\n",
    "This method performs the following steps:\n",
    "\n",
    "1. Validation: Checks if the vectorstore has been initialized. If not, it raises an error.\n",
    "\n",
    "2. Retriever Initialization: Converts the existing vector store into a retriever using the `as_retriever()` method.\n",
    "\n",
    "3. Feedback: Prints a confirmation message and displays the retriever's details.\n",
    "\n",
    "4. Return: Returns the retriever object for further use.\n",
    "  </text>\n",
    "</details>\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "  <summary>Understanding the Components</summary>\n",
    "  <text>\n",
    "\n",
    "#### 1. Vector Store\n",
    " * A vector store is a specialized database designed to store vector embeddings of documents. </br>\n",
    "In this context, the vector store (e.g., Chroma) holds the embeddings generated from your text documents.</br> \n",
    "These embeddings allow for efficient similarity searches based on vector distances.\n",
    "\n",
    "#### 2. Retriever\n",
    "* A retriever is an abstraction that facilitates the retrieval of relevant documents from the vector store based on a query. </br>\n",
    "By converting the vector store into a retriever, you enable the system to fetch documents that are semantically similar to a given input.\n",
    "\n",
    "#### Integration in a RAG Pipeline\n",
    "In a Retrieval-Augmented Generation pipeline, the retriever plays a crucial role:\n",
    "\n",
    "1. Query Embedding: A user's query is converted into an embedding using the same embedding model used for the documents.\n",
    "\n",
    "2. Similarity Search: The retriever searches the vector store for document embeddings that are closest to the query embedding.\n",
    "\n",
    "3. Contextual Retrieval: The most relevant documents are retrieved and provided as context to the language model.\n",
    "Niklas Heidloff\n",
    "\n",
    "4. Answer Generation: The language model generates a response based on the retrieved context and the original query.\n",
    "\n",
    "#### Customizing the Retriever\n",
    "The as_retriever() method can accept parameters to customize its behavior:\n",
    "\n",
    "* search_type: Defines the search strategy. Options include:\n",
    "\n",
    "    * \"similarity\": Retrieves documents based on vector similarity.\n",
    "    * \"mmr\" (Maximal Marginal Relevance): Balances relevance and diversity in the results.\n",
    "    * \"similarity_score_threshold\": Retrieves documents with a similarity score above a certain threshold.\n",
    "* search_kwargs: A dictionary of additional parameters, such as:\n",
    "    * k: Number of documents to retrieve.\n",
    "    * score_threshold: Minimum similarity score for a document to be considered relevant.\n",
    "\n",
    "  </text>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever set up from vectorstore.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Retriever details:</b> tags=['Chroma', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x133842920> search_kwargs={}"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x133842920>, search_kwargs={})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.setup_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "A retriever is configured to search the vector store for chunks most relevant to a given query, based on vector similarity.\n",
    "\n",
    "Expected Outcome:\n",
    "The retriever is ready to fetch relevant document chunks in response to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up QA Chain\n",
    "\n",
    "```python\n",
    "    def setup_qa_chain(self):\n",
    "        \"\"\"\n",
    "        Initializes the QA chain using a language model and the retriever.\n",
    "        Returns the QA chain object.\n",
    "        \"\"\"\n",
    "        if self.retriever is None:\n",
    "            raise ValueError(\"Retriever not initialized.\")\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=self.retriever)\n",
    "        print(\"QA chain initialized with LLM and retriever.\")\n",
    "        display(HTML(f\"<b>QA chain details:</b> {self.qa_chain}\"))\n",
    "        return self.qa_chain\n",
    "```\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Code explaination</summary>\n",
    "  <text>\n",
    "This method performs the following steps:\n",
    "\n",
    "1. Validation: Checks if the `retriever` has been initialized. If not, it raises an error.\n",
    "\n",
    "2. Language Model Initialization: Creates an instance of `ChatOpenAI` with the specified model and temperature settings.\n",
    "\n",
    "3. QA Chain Creation: Uses `RetrievalQA.from_chain_type` to create a question-answering chain that integrates the language model and retriever.\n",
    "\n",
    "4. Feedback: Prints a confirmation message and displays the QA chain's details.\n",
    "\n",
    "5. Return: Returns the QA chain object for further use.\n",
    "  </text>\n",
    "</details>\n",
    "</br>\n",
    "<details>\n",
    "  <summary>Understanding the Components</summary>\n",
    "  <text>\n",
    "\n",
    "#### 1. ChatOpenAI\n",
    "* This is a wrapper around OpenAI's chat models (like GPT-4) provided by LangChain. </br>\n",
    "It allows for easy integration of OpenAI's language models into LangChain workflows.\n",
    "\n",
    "    * model_name=\"gpt-4\": Specifies the use of OpenAI's GPT-4 model.\n",
    "    * temperature=0: Sets the randomness of the model's output. A temperature of 0 makes the output more deterministic and focused.\n",
    "\n",
    "#### 2. RetrievalQA.from_chain_type\n",
    "This method creates a RetrievalQA chain by combining a language model (llm) and a retriever. </br>\n",
    "* It simplifies the process of setting up a retrieval-augmented question-answering system.\n",
    "    * llm=llm: Passes the initialized language model to the chain.\n",
    "    * retriever=self.retriever: Passes the retriever, which is responsible for fetching relevant documents based on a query.\n",
    "\n",
    "#### Integration in a RAG Pipeline\n",
    "In a Retrieval-Augmented Generation pipeline, this QA chain plays a crucial role:\n",
    "    1. User Query: A user inputs a question.\n",
    "    2. Document Retrieval: The retriever searches the vector store for documents relevant to the query.\n",
    "    3. Answer Generation: The language model processes the retrieved documents and the original query to generate a comprehensive answer.\n",
    "\n",
    "#### Customization Options\n",
    "The RetrievalQA.from_chain_type method allows for various customizations:\n",
    "* chain_type: Specifies how to combine the documents. Options include:\n",
    "\n",
    "    * \"stuff\": Concatenates all documents and passes them to the language model.\n",
    "\n",
    "    * \"map_reduce\": Processes each document individually and then combines the results.\n",
    "\n",
    "    * \"refine\": Generates an initial answer and refines it with each additional document.\n",
    "\n",
    "* chain_type_kwargs: Allows for additional parameters, such as custom prompts.\n",
    "  </text>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA chain initialized with LLM and retriever.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>QA chain details:</b> verbose=False combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x133843ee0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x13384caf0>, root_client=<openai.OpenAI object at 0x133854a90>, root_async_client=<openai.AsyncOpenAI object at 0x133843d60>, model_name='gpt-4', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x133842920>, search_kwargs={})"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x133843ee0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x13384caf0>, root_client=<openai.OpenAI object at 0x133854a90>, root_async_client=<openai.AsyncOpenAI object at 0x133843d60>, model_name='gpt-4', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x133842920>, search_kwargs={}))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.setup_qa_chain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "A question-answering chain is established, combining the retriever and an OpenAI language model to generate answers based on retrieved document chunks.\n",
    "\n",
    "Expected Outcome:\n",
    "The QA chain is prepared to handle queries, retrieving relevant information and generating coherent responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Answer a Sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Query:</b> What is Retrieval-Augmented Generation?<br><b>Answer:</b> Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.answer_query(\"What is Retrieval-Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "This method processes a user query through the QA chain, retrieving pertinent information and generating an answer.\n",
    "\n",
    "Expected Outcome:\n",
    "A comprehensive answer to the question \"What is Retrieval-Augmented Generation?\" is generated, based on the content of your text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Evaluate the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Query 1:</b> Define RAG.<br><b>Expected:</b> Retrieval-Augmented Generation<br><b>Model Answer:</b> Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources.<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Query 2:</b> Explain vector databases.<br><b>Expected:</b> Vector databases store embeddings<br><b>Model Answer:</b> Vector databases are databases that store text embeddings, which are numerical representations of text that capture semantic meaning. These databases allow for efficient similarity searches, meaning they can quickly identify pieces of text that are similar based on their numerical representations.<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Evaluation Accuracy:</b> 50.00%"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_queries = [\"Define RAG.\", \"Explain vector databases.\"]\n",
    "sample_ground_truths = [\"Retrieval-Augmented Generation\", \"Vector databases store embeddings\"]\n",
    "rag.evaluate(sample_queries, sample_ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The system's performance is assessed by comparing its responses to known correct answers (ground truths) for a set of sample queries.\n",
    "\n",
    "Expected Outcome:\n",
    "Evaluation metrics (e.g., accuracy, precision) are computed, providing insights into the system's effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "class RAGClass:\n",
    "    def __init__(self, data_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the RAGClass with the path to the data file.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.documents = []\n",
    "        self.text_chunks = []\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self.qa_chain = None\n",
    "\n",
    "    def load_documents(self):\n",
    "        \"\"\"\n",
    "        Loads documents from the specified data path and stores them in self.documents.\n",
    "        Returns the loaded documents.\n",
    "        \"\"\"\n",
    "        loader = TextLoader(self.data_path)\n",
    "        self.documents = loader.load()\n",
    "        # print(f\"Loaded {len(self.documents)} documents.\")\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            preview = doc.page_content[:200].replace('\\n', '<br>')\n",
    "            display(HTML(f\"<b>Document {i+1} content preview:</b><br>{preview}{'...' if len(doc.page_content) > 200 else ''}<hr>\"))\n",
    "        return self.documents\n",
    "\n",
    "    def split_documents(self, chunk_size=500, chunk_overlap=50):\n",
    "        \"\"\"\n",
    "        Splits loaded documents into smaller chunks for processing.\n",
    "        Returns the list of text chunks.\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        self.text_chunks = text_splitter.split_documents(self.documents)\n",
    "        print(f\"Split documents into {len(self.text_chunks)} chunks.\")\n",
    "        for i, chunk in enumerate(self.text_chunks):\n",
    "            formatted_text = chunk.page_content.replace('. ', '.<br>')\n",
    "            display(HTML(f\"<b>Chunk {i+1}:</b><br>{formatted_text}<hr>\"))\n",
    "        return self.text_chunks\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        \"\"\"\n",
    "        Creates a vector store from the split text chunks using OpenAI embeddings.\n",
    "        Returns the vectorstore object.\n",
    "        \"\"\"\n",
    "        if not self.text_chunks:\n",
    "            raise ValueError(\"No text chunks found. Please split documents before creating the vector store.\")\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        self.vectorstore = Chroma.from_documents(self.text_chunks, embedding=embeddings)\n",
    "        print(\"Vectorstore created with embedded documents.\")\n",
    "        display(HTML(\"<b>Vectorstore Contents:</b>\"))\n",
    "        for i, doc in enumerate(self.text_chunks):\n",
    "            formatted_text = doc.page_content.replace('. ', '.<br>')\n",
    "            display(HTML(f\"<b>Document {i+1}:</b><br>{formatted_text}<hr>\"))\n",
    "        return self.vectorstore\n",
    "\n",
    "    def setup_retriever(self):\n",
    "        \"\"\"\n",
    "        Sets up a retriever from the vectorstore for similarity search.\n",
    "        Returns the retriever object.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\"Vectorstore not initialized.\")\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "        print(\"Retriever set up from vectorstore.\")\n",
    "        display(HTML(f\"<b>Retriever details:</b> {self.retriever}\"))\n",
    "        return self.retriever\n",
    "\n",
    "    def setup_qa_chain(self):\n",
    "        \"\"\"\n",
    "        Initializes the QA chain using a language model and the retriever.\n",
    "        Returns the QA chain object.\n",
    "        \"\"\"\n",
    "        if self.retriever is None:\n",
    "            raise ValueError(\"Retriever not initialized.\")\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=self.retriever)\n",
    "        print(\"QA chain initialized with LLM and retriever.\")\n",
    "        display(HTML(f\"<b>QA chain details:</b> {self.qa_chain}\"))\n",
    "        return self.qa_chain\n",
    "\n",
    "    def answer_query(self, query: str):\n",
    "        \"\"\"\n",
    "        Answers a query using the QA chain.\n",
    "        Returns the answer string.\n",
    "        \"\"\"\n",
    "        if self.qa_chain is None:\n",
    "            raise ValueError(\"QA chain not initialized.\")\n",
    "        result = self.qa_chain.run(query)\n",
    "        display(HTML(f\"<b>Query:</b> {query}<br><b>Answer:</b> {result}\"))\n",
    "        return result\n",
    "\n",
    "    def evaluate(self, queries: list, ground_truths: list):\n",
    "        \"\"\"\n",
    "        Evaluates the QA system using a list of queries and ground truths.\n",
    "        Returns the accuracy as a float.\n",
    "        \"\"\"\n",
    "        if len(queries) != len(ground_truths):\n",
    "            raise ValueError(\"Queries and ground truths must be of the same length.\")\n",
    "        if self.qa_chain is None:\n",
    "            raise ValueError(\"QA chain not initialized.\")\n",
    "        correct = 0\n",
    "        for idx, (query, truth) in enumerate(zip(queries, ground_truths)):\n",
    "            answer = self.qa_chain.run(query)\n",
    "            display(HTML(f\"<b>Query {idx+1}:</b> {query}<br><b>Expected:</b> {truth}<br><b>Model Answer:</b> {answer}<hr>\"))\n",
    "            if truth.lower() in answer.lower():\n",
    "                correct += 1\n",
    "        accuracy = correct / len(queries)\n",
    "        display(HTML(f\"<b>Evaluation Accuracy:</b> {accuracy * 100:.2f}%\"))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents.\n",
      "Split documents into 2 chunks.\n",
      "Vectorstore created with embedded documents.\n",
      "Retriever set up from vectorstore.\n",
      "QA chain initialized with LLM and retriever.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/v36xqm_16qlckbh0hgrshdr40000gn/T/ipykernel_47190/338421976.py:49: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = self.qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Retrieval-Augmented Generation?\n",
      "Answer: Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources. This allows the models to generate more informed and accurate responses.\n",
      "Evaluation Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This will load variables from the .env file into the environment\n",
    "\n",
    "# Now you can access the API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the RAG class with the path to your data\n",
    "rag = RAGClass(data_path=\"my_text_file.txt\")\n",
    "\n",
    "# Load and process documents\n",
    "rag.load_documents()\n",
    "rag.split_documents()\n",
    "rag.create_vectorstore()\n",
    "rag.setup_retriever()\n",
    "rag.setup_qa_chain()\n",
    "\n",
    "# Answer a sample query\n",
    "rag.answer_query(\"What is Retrieval-Augmented Generation?\")\n",
    "\n",
    "# Evaluate the system with sample queries and ground truths\n",
    "sample_queries = [\"Define RAG.\", \"Explain vector databases.\"]\n",
    "sample_ground_truths = [\"Retrieval-Augmented Generation\", \"Vector databases store embeddings\"]\n",
    "rag.evaluate(sample_queries, sample_ground_truths)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
