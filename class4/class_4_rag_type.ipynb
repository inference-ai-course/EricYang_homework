{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b25e3c5",
   "metadata": {},
   "source": [
    "### RAG chunking strategies: fixed-size vs semantic vs mixed\n",
    "\n",
    "This notebook implements and compares three chunking strategies for RAG:\n",
    "- Fixed-size (word-based window with overlap)\n",
    "- Semantic (sentence-aware, coherence-driven)\n",
    "- Mixed (semantic-first, fixed-size fallback with overlap)\n",
    "\n",
    "We report chunk stats and an approximate internal coherence metric to contrast trade-offs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e9262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Callable\n",
    "\n",
    "# Lightweight sentence split without external deps\n",
    "import re\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "\tid: int\n",
    "\ttext: str\n",
    "\tstart_char: int\n",
    "\tend_char: int\n",
    "\n",
    "\n",
    "def read_text_file(path: str | Path) -> str:\n",
    "\tp = Path(path)\n",
    "\tif not p.exists():\n",
    "\t\traise FileNotFoundError(f\"File not found: {p}\")\n",
    "\ttext = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\treturn text.strip()\n",
    "\n",
    "\n",
    "def split_sentences(text: str) -> List[Tuple[str, Tuple[int, int]]]:\n",
    "\tif not text:\n",
    "\t\treturn []\n",
    "\t# Simple sentence boundary on ., !, ? while keeping indices\n",
    "\tpattern = re.compile(r\"(?<=[.!?])\\s+\")\n",
    "\tparts = pattern.split(text)\n",
    "\tspans: List[Tuple[str, Tuple[int, int]]] = []\n",
    "\tcursor = 0\n",
    "\tfor part in parts:\n",
    "\t\tstart = text.find(part, cursor)\n",
    "\t\tend = start + len(part)\n",
    "\t\tspans.append((part, (start, end)))\n",
    "\t\tcursor = end\n",
    "\treturn spans\n",
    "\n",
    "\n",
    "def window_words(words: List[str], size: int, overlap: int) -> Iterable[Tuple[int, int]]:\n",
    "\tif size <= 0:\n",
    "\t\traise ValueError(\"size must be > 0\")\n",
    "\tif overlap < 0 or overlap >= size:\n",
    "\t\traise ValueError(\"overlap must be in [0, size-1]\")\n",
    "\tstart = 0\n",
    "\twhile start < len(words):\n",
    "\t\tend = min(start + size, len(words))\n",
    "\t\tyield start, end\n",
    "\t\tif end == len(words):\n",
    "\t\t\tbreak\n",
    "\t\tstart = end - overlap\n",
    "\n",
    "\n",
    "def fixed_size_chunk(text: str, size_words: int = 200, overlap_words: int = 40) -> List[Chunk]:\n",
    "\tif not text:\n",
    "\t\treturn []\n",
    "\twords = text.split()\n",
    "\tchunks: List[Chunk] = []\n",
    "\tfor i, (w_start, w_end) in enumerate(window_words(words, size_words, overlap_words)):\n",
    "\t\tchunk_words = words[w_start:w_end]\n",
    "\t\tchunk_text = \" \".join(chunk_words)\n",
    "\t\tstart_char = text.find(chunk_text)  # approximate\n",
    "\t\tend_char = start_char + len(chunk_text)\n",
    "\t\tchunks.append(Chunk(i, chunk_text, start_char, end_char))\n",
    "\treturn chunks\n",
    "\n",
    "\n",
    "def semantic_chunk(text: str, target_chars: int = 800, max_chars: int = 1200) -> List[Chunk]:\n",
    "\tif not text:\n",
    "\t\treturn []\n",
    "\tsentences = split_sentences(text)\n",
    "\tif not sentences:\n",
    "\t\treturn [Chunk(0, text, 0, len(text))]\n",
    "\tchunks: List[Chunk] = []\n",
    "\tcurrent: List[str] = []\n",
    "\tcurrent_start = sentences[0][1][0]\n",
    "\tfor sent, (s_start, s_end) in sentences:\n",
    "\t\tcandidate = (\" \".join(current) + (\" \" if current else \"\") + sent)\n",
    "\t\tif len(candidate) <= max_chars:\n",
    "\t\t\tcurrent.append(sent)\n",
    "\t\t\tcontinue\n",
    "\t\t# finalize current\n",
    "\t\tchunk_text = \" \".join(current).strip()\n",
    "\t\tif chunk_text:\n",
    "\t\t\tchunks.append(Chunk(len(chunks), chunk_text, current_start, current_start + len(chunk_text)))\n",
    "\t\t# reset with current sentence\n",
    "\t\tcurrent = [sent]\n",
    "\t\tcurrent_start = s_start\n",
    "\t# flush\n",
    "\tchunk_text = \" \".join(current).strip()\n",
    "\tif chunk_text:\n",
    "\t\tchunks.append(Chunk(len(chunks), chunk_text, current_start, current_start + len(chunk_text)))\n",
    "\t# second pass: try to split oversized by naive midpoint if needed\n",
    "\trefined: List[Chunk] = []\n",
    "\tfor ch in chunks:\n",
    "\t\tif len(ch.text) <= max_chars:\n",
    "\t\t\trefined.append(Chunk(len(refined), ch.text, ch.start_char, ch.end_char))\n",
    "\t\t\tcontinue\n",
    "\t\tmid = len(ch.text) // 2\n",
    "\t\trefined.append(Chunk(len(refined), ch.text[:mid].rstrip(), ch.start_char, ch.start_char + mid))\n",
    "\t\trefined.append(Chunk(len(refined), ch.text[mid:].lstrip(), ch.start_char + mid, ch.end_char))\n",
    "\treturn refined\n",
    "\n",
    "\n",
    "def mixed_chunk(text: str, target_chars: int = 800, fallback_words: int = 200, overlap_words: int = 40) -> List[Chunk]:\n",
    "\tchunks = semantic_chunk(text, target_chars=target_chars, max_chars=int(target_chars * 1.5))\n",
    "\tif chunks:\n",
    "\t\treturn chunks\n",
    "\treturn fixed_size_chunk(text, size_words=fallback_words, overlap_words=overlap_words)\n",
    "\n",
    "\n",
    "def chunk_stats(chunks: List[Chunk]) -> dict:\n",
    "\tif not chunks:\n",
    "\t\treturn {\"num_chunks\": 0, \"avg_chars\": 0, \"avg_words\": 0}\n",
    "\tlengths = [len(c.text) for c in chunks]\n",
    "\tword_counts = [len(c.text.split()) for c in chunks]\n",
    "\treturn {\n",
    "\t\t\"num_chunks\": len(chunks),\n",
    "\t\t\"avg_chars\": sum(lengths) / len(lengths),\n",
    "\t\t\"avg_words\": sum(word_counts) / len(word_counts),\n",
    "\t}\n",
    "\n",
    "\n",
    "def jaccard_similarity(a: set[str], b: set[str]) -> float:\n",
    "\tif not a and not b:\n",
    "\t\treturn 1.0\n",
    "\tif not a or not b:\n",
    "\t\treturn 0.0\n",
    "\treturn len(a & b) / len(a | b)\n",
    "\n",
    "\n",
    "def coherence_score(chunk: Chunk) -> float:\n",
    "\t# crude proxy: measure overlap of unique words between first and second half\n",
    "\ttext = chunk.text\n",
    "\tif not text:\n",
    "\t\treturn 0.0\n",
    "\tmid = max(1, len(text) // 2)\n",
    "\tfirst = set(re.findall(r\"[A-Za-z0-9]+\", text[:mid].lower()))\n",
    "\tsecond = set(re.findall(r\"[A-Za-z0-9]+\", text[mid:].lower()))\n",
    "\treturn jaccard_similarity(first, second)\n",
    "\n",
    "\n",
    "def evaluate_chunks(chunks: List[Chunk]) -> dict:\n",
    "\tstats = chunk_stats(chunks)\n",
    "\tcoherence = [coherence_score(c) for c in chunks] if chunks else []\n",
    "\tstats[\"avg_coherence\"] = sum(coherence) / len(coherence) if coherence else 0.0\n",
    "\treturn stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df13a916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Basic tests for core helpers\n",
    "\n",
    "def test_window_words():\n",
    "\twords = [str(i) for i in range(10)]\n",
    "\tspans = list(window_words(words, size=4, overlap=1))\n",
    "\tassert spans == [(0,4),(3,7),(6,10)]\n",
    "\n",
    "\n",
    "def test_split_sentences():\n",
    "\ttext = \"A. B? C! D\"\n",
    "\tspans = split_sentences(text)\n",
    "\tassert len(spans) == 4\n",
    "\tassert spans[0][0].strip() == \"A.\"\n",
    "\tassert spans[1][0].strip() == \"B?\"\n",
    "\n",
    "\n",
    "def test_fixed_size_chunk():\n",
    "\ttext = \"one two three four five six seven eight nine ten\"\n",
    "\tchunks = fixed_size_chunk(text, size_words=3, overlap_words=1)\n",
    "\tassert len(chunks) == 5\n",
    "\tassert chunks[0].text.split() == [\"one\",\"two\",\"three\"]\n",
    "\tassert chunks[1].text.split() == [\"three\",\"four\",\"five\"]\n",
    "\n",
    "\n",
    "def test_semantic_chunk_small():\n",
    "\ttext = \"A short sentence. Another one. And the last!\"\n",
    "\tchunks = semantic_chunk(text, target_chars=20, max_chars=30)\n",
    "\tassert len(chunks) >= 2\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "\ttest_window_words()\n",
    "\ttest_split_sentences()\n",
    "\ttest_fixed_size_chunk()\n",
    "\ttest_semantic_chunk_small()\n",
    "\tprint(\"All tests passed\")\n",
    "\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e82facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750 chars from /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/class4/my_text_file.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources.\\n\\nText embeddings are numerical representations of text that capture semantic meaning. They are used to measure similarity between pieces of text.\\n\\nVector databases store these embeddings and allow for efficient similarity searches.\\n\\nLangChain is a framework that facilitates the de'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load source text\n",
    "\n",
    "BASE_DIR = \"/Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI\"\n",
    "TEXT_PATH = f\"{BASE_DIR}/class4/my_text_file.txt\"\n",
    "\n",
    "try:\n",
    "\ttext = read_text_file(TEXT_PATH)\n",
    "\tprint(f\"Loaded {len(text)} chars from {TEXT_PATH}\")\n",
    "except FileNotFoundError as e:\n",
    "\tprint(str(e))\n",
    "\ttext = \"\"\n",
    "\n",
    "text[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de98e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixed': {'num_chunks': 1, 'avg_chars': 746.0, 'avg_words': 104.0}, 'semantic': {'num_chunks': 1, 'avg_chars': 746.0, 'avg_words': 104.0}, 'mixed': {'num_chunks': 1, 'avg_chars': 746.0, 'avg_words': 104.0}}\n"
     ]
    }
   ],
   "source": [
    "# Generate chunks for each strategy\n",
    "\n",
    "if text:\n",
    "\tfixed_chunks = fixed_size_chunk(text, size_words=180, overlap_words=40)\n",
    "\tsemantic_chunks = semantic_chunk(text, target_chars=900, max_chars=1350)\n",
    "\tmixed_chunks = mixed_chunk(text, target_chars=900, fallback_words=180, overlap_words=40)\n",
    "\tprint({\n",
    "\t\t\"fixed\": chunk_stats(fixed_chunks),\n",
    "\t\t\"semantic\": chunk_stats(semantic_chunks),\n",
    "\t\t\"mixed\": chunk_stats(mixed_chunks)\n",
    "\t})\n",
    "else:\n",
    "\tfixed_chunks = []\n",
    "\tsemantic_chunks = []\n",
    "\tmixed_chunks = []\n",
    "\tprint(\"No text loaded; stats unavailable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72b1bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixed': {'num_chunks': 1, 'avg_chars': 746.0, 'avg_words': 104.0, 'avg_coherence': 0.20270270270270271}, 'semantic': {'num_chunks': 1, 'avg_chars': 746.0, 'avg_words': 104.0, 'avg_coherence': 0.20270270270270271}, 'mixed': {'num_chunks': 1, 'avg_chars': 746.0, 'avg_words': 104.0, 'avg_coherence': 0.20270270270270271}}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate chunk quality via crude coherence metric\n",
    "\n",
    "if text:\n",
    "\tfixed_eval = evaluate_chunks(fixed_chunks)\n",
    "\tsemantic_eval = evaluate_chunks(semantic_chunks)\n",
    "\tmixed_eval = evaluate_chunks(mixed_chunks)\n",
    "\tprint({\n",
    "\t\t\"fixed\": fixed_eval,\n",
    "\t\t\"semantic\": semantic_eval,\n",
    "\t\t\"mixed\": mixed_eval\n",
    "\t})\n",
    "else:\n",
    "\tprint(\"No text loaded; evaluation unavailable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e58592c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-size preview:\n",
      "[id=0] chars=746 coherence=0.203\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources. Text embeddings are numerical representations of text that capture semantic meani\n",
      "------------------------------------------------------------\n",
      "Semantic preview:\n",
      "[id=0] chars=746 coherence=0.203\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources. Text embeddings are numerical representations of text that capture semantic meani\n",
      "------------------------------------------------------------\n",
      "Mixed preview:\n",
      "[id=0] chars=746 coherence=0.203\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources. Text embeddings are numerical representations of text that capture semantic meani\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Preview a few chunks from each strategy\n",
    "\n",
    "def preview(chunks: List[Chunk], n: int = 2):\n",
    "\tfor c in chunks[:n]:\n",
    "\t\tprint(f\"[id={c.id}] chars={len(c.text)} coherence={coherence_score(c):.3f}\")\n",
    "\t\tprint(c.text[:300].replace(\"\\n\",\" \"))\n",
    "\t\tprint(\"-\"*60)\n",
    "\n",
    "if text:\n",
    "\tprint(\"Fixed-size preview:\")\n",
    "\tpreview(fixed_chunks)\n",
    "\tprint(\"Semantic preview:\")\n",
    "\tpreview(semantic_chunks)\n",
    "\tprint(\"Mixed preview:\")\n",
    "\tpreview(mixed_chunks)\n",
    "else:\n",
    "\tprint(\"No text loaded; preview unavailable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf836c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixed': {'num_chunks': 4, 'avg_chars': 240.0, 'avg_words': 33.5}, 'semantic': {'num_chunks': 3, 'avg_chars': 248.0, 'avg_words': 34.666666666666664}, 'mixed': {'num_chunks': 3, 'avg_chars': 248.0, 'avg_words': 34.666666666666664}}\n"
     ]
    }
   ],
   "source": [
    "# Re-tune parameters to exaggerate differences and regenerate\n",
    "\n",
    "if text:\n",
    "\t# Smaller windows for clearer contrast on short docs\n",
    "\tFIXED_SIZE_WORDS = 40\n",
    "\tFIXED_OVERLAP_WORDS = 10\n",
    "\tSEM_TARGET_CHARS = 250\n",
    "\tSEM_MAX_CHARS = 350\n",
    "\tMIX_TARGET_CHARS = SEM_TARGET_CHARS\n",
    "\tMIX_FALLBACK_WORDS = FIXED_SIZE_WORDS\n",
    "\tMIX_OVERLAP_WORDS = FIXED_OVERLAP_WORDS\n",
    "\n",
    "\tfixed_chunks = fixed_size_chunk(text, size_words=FIXED_SIZE_WORDS, overlap_words=FIXED_OVERLAP_WORDS)\n",
    "\tsemantic_chunks = semantic_chunk(text, target_chars=SEM_TARGET_CHARS, max_chars=SEM_MAX_CHARS)\n",
    "\tmixed_chunks = mixed_chunk(text, target_chars=MIX_TARGET_CHARS, fallback_words=MIX_FALLBACK_WORDS, overlap_words=MIX_OVERLAP_WORDS)\n",
    "\n",
    "\tprint({\n",
    "\t\t\"fixed\": chunk_stats(fixed_chunks),\n",
    "\t\t\"semantic\": chunk_stats(semantic_chunks),\n",
    "\t\t\"mixed\": chunk_stats(mixed_chunks)\n",
    "\t})\n",
    "else:\n",
    "\tprint(\"No text loaded; skip retune\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ddb116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-size:\n",
      "[id=0] span=(-1,302) words=40 coherence=0.118\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources. T\n",
      "------------------------------------------------------------\n",
      "[id=1] span=(-1,284) words=40 coherence=0.118\n",
      "embeddings are numerical representations of text that capture semantic meaning. They are used to measure similarity between pieces of text. Vector databases store these embeddings and allow for efficient similarity searc\n",
      "------------------------------------------------------------\n",
      "[id=2] span=(-1,278) words=40 coherence=0.051\n",
      "searches. LangChain is a framework that facilitates the development of applications powered by language models. It provides tools for integrating LLMs with external data sources. Evaluation metrics for RAG systems includ\n",
      "------------------------------------------------------------\n",
      "[id=3] span=(657,750) words=14 coherence=0.000\n",
      "include precision, recall, and F1 score, which assess the quality of the generated responses.\n",
      "------------------------------------------------------------\n",
      "Semantic:\n",
      "[id=0] span=(0,303) words=40 coherence=0.118\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources. T\n",
      "------------------------------------------------------------\n",
      "[id=1] span=(305,618) words=45 coherence=0.047\n",
      "They are used to measure similarity between pieces of text. Vector databases store these embeddings and allow for efficient similarity searches. LangChain is a framework that facilitates the development of applications p\n",
      "------------------------------------------------------------\n",
      "[id=2] span=(622,750) words=19 coherence=0.000\n",
      "Evaluation metrics for RAG systems include precision, recall, and F1 score, which assess the quality of the generated responses.\n",
      "------------------------------------------------------------\n",
      "Mixed:\n",
      "[id=0] span=(0,363) words=50 coherence=0.075\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources. T\n",
      "------------------------------------------------------------\n",
      "[id=1] span=(366,619) words=35 coherence=0.029\n",
      "Vector databases store these embeddings and allow for efficient similarity searches. LangChain is a framework that facilitates the development of applications powered by language models. It provides tools for integrating\n",
      "------------------------------------------------------------\n",
      "[id=2] span=(622,750) words=19 coherence=0.000\n",
      "Evaluation metrics for RAG systems include precision, recall, and F1 score, which assess the quality of the generated responses.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show more chunks with boundaries and indexes\n",
    "\n",
    "def preview_with_bounds(title: str, chunks: List[Chunk], n: int = 5):\n",
    "\tprint(title)\n",
    "\tfor c in chunks[:n]:\n",
    "\t\tprint(f\"[id={c.id}] span=({c.start_char},{c.end_char}) words={len(c.text.split())} coherence={coherence_score(c):.3f}\")\n",
    "\t\tprint(c.text[:220].replace(\"\\n\",\" \"))\n",
    "\t\tprint(\"-\"*60)\n",
    "\n",
    "if text:\n",
    "\tpreview_with_bounds(\"Fixed-size:\", fixed_chunks)\n",
    "\tpreview_with_bounds(\"Semantic:\", semantic_chunks)\n",
    "\tpreview_with_bounds(\"Mixed:\", mixed_chunks)\n",
    "else:\n",
    "\tprint(\"No text loaded; preview unavailable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc8fa8",
   "metadata": {},
   "source": [
    "**span**: \n",
    "zero-based character offsets into the original source text. The pair (start_char, end_char) indicates the substring boundaries in the full text for that chunk.\n",
    "\n",
    "**coherence**:\n",
    " a quick intra-chunk topical consistency proxy in [0, 1]. It computes the Jaccard similarity between the sets of unique tokens in the first half vs the second half of the chunk. Higher ≈ more similar vocabulary across halves → likely more self-consistent. It’s crude and can be noisy on very short chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ffe43",
   "metadata": {},
   "source": [
    "1. Why mix them?\n",
    "\n",
    "    ### Semantic first: \n",
    "    Break the text at natural meaning boundaries (e.g., paragraphs, section headers, Q&A pairs).\n",
    "\n",
    "    ### Fixed-size second: \n",
    "    Apply a size cap (e.g., 800–1,000 tokens) with a small overlap (e.g., 10–20%) so no chunk exceeds the embedding model’s sweet spot.\n",
    "\n",
    "    This gives you:\n",
    "\n",
    "    - **Context preservation** → because you’re not slicing mid-sentence or mid-clause\n",
    "\n",
    "    - **Uniform chunk sizes** → which improves embedding consistency and avoids overly large chunks that bloat retrieval latency\n",
    "\n",
    "2. A common real-world pipeline\n",
    "\n",
    "    1. Clean & normalize text (OCR fixes, whitespace, etc.)\n",
    "\n",
    "    2. Semantic segmentation (by headings, sentences, topic shifts)\n",
    "\n",
    "    3. Token-based capping (truncate or further split if >N tokens)\n",
    "\n",
    "    4. Overlap windowing (to avoid losing linking sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5aa88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8320e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
