# Retrieval Report (2025-08-18)

## Query: What is the role of attention in transformer models?
**[1] Continuous_Bangla_Sign_Language_Translation_Mitigating_the_Expense_of_Gloss_Annotation_with_the_Assistance_of_Graph (chunk 14) — dist=0.6482**
> a matrix operation: Attention(Q, K, V ) = softmax (QKT √dk ) V The horizontal axis is the axis over which the softmax function is applied. 2.1.2.1 Multi-Head Attention In transformer models, a group of matrices (WQ, WK, WV ) is called as attention head, and each layer contains multiple attention heads. The purpose of each of the attention head is to concentrate on these tokens are pertinent to a specific token, and with several attention heads, the task according to various explanation of ”pertinence” is accomplished by the model. Moreover, the importance field that represents pertinence can become increasingly expanded across consecutive layers. Several attention heads in transformer models code meaningful pertinence rela- tionships that are intuitive to normal humans. For instance, some  ...

**[2] Continuous_Bangla_Sign_Language_Translation_Mitigating_the_Expense_of_Gloss_Annotation_with_the_Assistance_of_Graph (chunk 13) — dist=0.8587**
> inputs, which are passed to the subsequent encoder layer as inputs. Similarly, the contextual information of the encodings is utilized by every decoder layer to produce the output sequence, utilizing attention mechanisms. The attention mechanism weighs the relevance of each input part to generate the output. An attention mechanism is implemented independently in the layer of each decoder that extracts information from the output of the decoder that came before and only then information is drawn via means of encodings. Both the layers of encoding and decoding include extra processing steps such as feed- forward neural networks, residual connections, and layer normalization to enhance the output. 8 Chapter 2. Preliminaries 9 Figure 2.1: Basic Architecture of Trasformer[1] 2.1.2 Attention usi ...

**[3] Continuous_Bangla_Sign_Language_Translation_Mitigating_the_Expense_of_Gloss_Annotation_with_the_Assistance_of_Graph (chunk 5) — dist=0.8593**
> . . . . . . . 26 3.5 Overview of the proposed TSPNet workflow, which directly pro- duces spoken language translations from sign language videos[5] . . 27 3.6 A Transformer is utilized to handle the sequence of input video and generate the corresponding sequence of output text.[6] . . . . . . . 29 3.7 Visualisation of the attention map in three different SLT models’ shallow encoder layers. As seen in (a), one of gloss’s key functions is to give the model alignment information so that it can concentrate on substantially more crucial local areas. The conventional method for calculating attention faces diﬀiculty in accurately converging to the correct position when the supervision signal of the gloss is lost, as illustrated in (b) By incorporating an inductive bias, which might be somewhat sub ...

---

## Query: How do instruction-tuned LLMs differ from base models?
**[1] SSRL_Self-Search_Reinforcement_Learning (chunk 28) — dist=1.1832**
> ample, <answer> Beijing </answer>. Question: Table 12: Instruction for repeated sampling. A.1.2 INSTRUCTIONS FOR LLM PROVIDING INFORMATION We use the instruction in Table 13 when querying LLM to provide information. Given a query, you need to imitate the style of the following demos and generate five useful documents for the query. [EXAMPLE] You should generate documents that can help the user find the answer. Each document should contain about 30 words. You must directly output the English documents and not output any other texts. Query: query Useful Output: Table 13: Instruction for LLM providing information. A.2 DETAILED RESULTS We introduce the results of repeated sampling of seven benchmarks, across 16 models, in Figure 12. We also list the simulated parameters for each model in Table ...

**[2] Improving_Generative_Cross-lingual_Aspect-Based_Sentiment_Analysis_with_Constrained_Decoding (chunk 24) — dist=1.1943**
> similar to the mT5 model with constrained decoding in some cases. Fine-tuned LLMs generally perform better than mT5 when English is the target language. Nev- ertheless, using English as the target language is impractical and uncommon in real-world scenarios where English is typically the source language. For the more common scenario where English is the source language, mT5 often outper- 24 J. Šmíd et al. forms LLMs, sometimes by more than 10% for certain language combinations. The choice of LLM is crucial. Only Orca 2 13B achieves comparable results to mT5 among the evaluated LLMs. Since LLMs are predominantly pre-trained on English data7 and multilingual open-source LLMs are still evolving, we recom- mend using mT5 with constrained decoding. Additionally, fine-tuning LLMs on consumer GPU ...

**[3] Inductive_Bias_Extraction_and_Matching_for_LLM_Prompts (chunk 2) — dist=1.2114**
> applied to tasks that existed prior to LLMs being introduced. The motivation for applying LLMs to these older tasks may be the desire to improve efficiency in execution speed, improve accessibility to non-technical users, or im- prove the state-of-the-art in accuracy. This broad spectrum of LLM applications is what has moti- vated us to evaluate IBEaM by generating numeric scores and using them as a proxy for performing classification and ranking tasks. One method of improving LLM performance on tasks is to perform chain of thought reasoning, where the LLM is walked through the process of solving one or more examples in the prompt prior to being asked to evaluate an instance (Wei et al., 2022). Another method of improving LLM per- formance is to perform prompt engineering (Chen et al., 202 ...

---

## Query: Techniques for mitigating hallucinations in RAG systems
**[1] Prompt-Response_Semantic_Divergence_Metrics_for_Faithfulness_Hallucination_and_Misalignment_Detection_in_Large_Language_ (chunk 1) — dist=1.1424**
> created by AI assistants were caught and removed by the author. All remaining errors are the author’s own. The views expressed herein are those of the author and do not necessarily reflect the views of his employer. Email for correspondence: ighalp@gmail.com. 1 Some researchers have proposed that the term confabulation is more appropriate than ”hallucina- tion” for describing these LLM failures [6]. In psychiatry, ”hallucinations” typically refer to sensory- related phenomena, while ”confabulations” are false or distorted memories that a person believes to be true. This occurs when an individual fills in gaps in their memory with fabricated details without the conscious intent to deceive. This psychiatric definition makes ”confabulation” a meaningful synonym for the kind of contextual misa ...

**[2] Prompt-Response_Semantic_Divergence_Metrics_for_Faithfulness_Hallucination_and_Misalignment_Detection_in_Large_Language_ (chunk 26) — dist=1.3912**
> Abdaljalil, L. Kort, and Y. Singer. SINdex: Semantic inconsistency index for hallucination detection in LLMs. In Proceedings of the 13th International Conference on Learning Representations, 2025. [2] Agrawal, G., Kumarage, T., Alghami, Z., and Liu, H. (2024). Can Large Language Models Under- stand Context? arXiv:2402.00858 [cs.CL]. [3] Aharoni, R. and Goldberg, Y. (2020). Unsupervised Domain Clusters in Pretrained Language Models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). [4] M. Cossio (2025). A comprehensive taxonomy of hallucinations in large language models. arXiv:https://arxiv.org/abs/2508.01781 [5] Ethayarajh, K. (2019). How Contextual are Contextualized Word Representations?. In Proceedings of the 2019 Conference on Empirical M ...

**[3] Using_Large_Language_Models_to_Measure_Symptom_Severity_in_Patients_At_Risk_for_Schizophrenia (chunk 7) — dist=1.4070**
> same evaluation. Furthermore, the PSYCHS instrument is too new to have substantial outside data, and in fact, we found LLM is not even familiar enough with the PSYCHS to be able to list out its component items. Consequently, we cannot yet determine whether LLMs can assess these disease-specific scales, which may be more sensitive for assessing the conversion to psychosis than the BPRS. However, the BPRS and its components have already shown utility in predictive modeling in the CHR population9–14, which is encouraging for its future integration into outcomes-based risk calculators. Promisingly, certain key symptoms noted by studies such as NAPLS that are associated with conversion to psychosis (e.g. suspiciousness3, hallucinations31) were predicted with high accuracy by the LLM. Sparse lon ...

---

## Query: What datasets are used for multilingual NLU benchmarks?
**[1] Improving_Generative_Cross-lingual_Aspect-Based_Sentiment_Analysis_with_Constrained_Decoding (chunk 3) — dist=0.8430**
> art results for existing cross-lingual ABSA tasks and eliminates the need for external translation tools. To the best of our knowledge, this study repre- sents the first application of sequence-to-sequence models and large language models for cross-lingual ABSA. – We conduct extensive experiments on benchmark datasets across seven lan- guages, evaluating performance on six distinct ABSA tasks. In addition to improving results on established cross-lingual and monolingual ABSA tasks, we pioneer the evaluation of several previously unexplored cross- lingual ABSA tasks. – Our methodology supports multi-tasking capabilities, enabling the simulta- neous solution of multiple ABSA tasks using a single model. Constrained decoding proves particularly effective in multi-tasking settings, consistently ...

**[2] Large_Language_Models_for_Summarizing_Czech_Historical_Documents_and_Beyond (chunk 2) — dist=0.8915**
> scientific papers and their abstracts sourced from arXiv. It has been cleaned and format- ted to ensure standardization, with sections like fig- ures and tables removed. BOOKSUM (Kryscinski et al., 2022) is a dataset tailored for summarizing long texts like novels, plays, and stories, with summaries provided at paragraph, chapter, and book levels. Texts and summaries were sourced from Project Gutenberg and other web archives, supporting both extractive and abstractive summarization. 3.2 Multilingual Datasets XLSum (Hasan et al., 2021) provides over one mil- lion article-summary pairs across 44 languages, rang- ing from low-resource languages like Bengali and Swahili to high-resource languages such as English and Russian. Extracted from various BBC sites, this dataset is a valuable resource ...

**[3] Estimating_Machine_Translation_Difficulty (chunk 19) — dist=0.9062**
> score as an independent training instance. Since human scores are assigned to individual translations, multiple annotations may exist for the same source text. We do not combine these scores in any way but include them all in the training data for both DA and MQM stages. Train- ing hyperparameters match those used by Perrella et al. (2024) for Sentinel-MQM. All models were trained using a single NVIDIA GeForce RTX 4090 GPU. The estimated training time is approximately three GPU hours for the first (DA) stage and one GPU hour for the second (MQM) fine-tuning stage. These estimates apply to both Sentinel-MQM-24 and Sentinel-MQM-25. C Implementation Details • For the word rarity heuristic, we com- pute word frequencies using the wordfreq Python library (Speer, 2022). • For the syntactic compl ...

---

## Query: Contrastive learning methods for sentence embeddings
**[1] Reverse_Physician-AI_Relationship_Full-process_Clinical_Diagnosis_Driven_by_a_Large_Language_Model (chunk 32) — dist=0.8174**
> a query seeking medical knowledge, it represents the query and each paragraph in the corpus as vectors. The matching score between a query and a paragraph is determined by calculating the similarity between their vectors such as dot product. The paragraphs are then ranked in descending order based on their matching scores, and the top-k paragraphs are selected as the search results for the given query. To make the search model more accurate in the vector representation of medical text, we train Gemma-2B on large-scale medical data in contrastive learning method, which will be introduced below. Data Collection and Process We collect a large amount of text data from the medical domain to train our model. The training dataset consists of 11M medical articles come from medical textbooks, publi ...

**[2] Large_Language_Models_for_Summarizing_Czech_Historical_Documents_and_Beyond (chunk 1) — dist=1.0664**
> extractive ones. Extractive sum- marization selects the most representative sentences from the source document, while abstractive summa- 1corpora.kiv.zcu.cz/posel od cerchova/ Accepted at ICAART 2025. Official version: doi.org/10.5220/0013374100003890 arXiv:2508.10368v1 [cs.CL] 14 Aug 2025 rization generates summaries composed of newly cre- ated sentences. Early summarization methods were extractive ones and relied on statistical and graph-based meth- ods like TF-IDF (Term Frequency-Inverse Document Frequency) (Christian et al., 2016), which scores sen- tence importance based on term frequency relative to rarity across a corpus. Similarly, TextRank (Mihal- cea and Tarau, 2004) represents sentences as nodes in a graph and ranks them using the PageRank algo- rithm (Page et al., 1999). Neural ...

**[3] Advancing_Cross-lingual_Aspect-Based_Sentiment_Analysis_with_LLMs_and_Constrained_Decoding_for_Sequence-to-Sequence_Mode (chunk 2) — dist=1.0679**
> compound task, i.e. it focuses on extracting more than one sentiment element si- multaneously. Early methods relied on translation and word alignment tools like FastAlign (Dyer et al., 2013), with quality improvements through instance selection (Klinger and Cimiano, 2015) or constrained translation (Lambert, 2015). Others used cross- lingual embeddings trained on bilingual corpora for language-independent ABSA (Lambert, 2015; Barnes et al., 2016; Akhtar et al., 2018; Wang and Pan, 2018; Jebbara and Cimiano, 2019). Recent work focus on multilingual Transformer-based (Vaswani et al., 2017) encoder-only models, such as mBERT (De- vlin et al., 2019) and XLM-R (Conneau et al., 2020) combined with machine translation, with ad- ditional enhancements from parameter warm-up (Li et al., 2020), disti ...

---
